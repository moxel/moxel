{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Moxel Documentation.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-moxel-documentation", 
            "text": "", 
            "title": "Welcome to Moxel Documentation."
        }, 
        {
            "location": "/install/cli/", 
            "text": "Installation Guide\n\n\nThis guide explains how to install Moxel.\n\n\nWith Python PIP\n\n\nThe easiest way to install Moxel regardless of the operating system is through \npip\n. \n\n\npip install moxel\n\n\n\n\n\n\n\n\n\n\nThis step will install two things:\n\n\n\n\nMoxel CLI\n: a command-line tool to deploy machine learning models.\n\n\nMoxel Python Client\n: this is a Python client to use Moxel models, through \nimport moxel\n.\n\n\n\n\nAfter pip install, to test if the CLI is working, run\n\n\nmoxel login\n\n\n\n\n\n\n\n\n\n\nIf you are using a local computer, this step will open a web browser; if you currently ssh into a remote cluster, \nmoxel\n will ask for username and passwords in headless mode.\n\n\nYou can also checkout the moxel version by \n\n\nmoxel version\n\n\n\n\nManually Install CLI\n\n\nMoxel CLI is released as standalone binaries. You can download them through\n\n\ncurl -o moxel http://beta.moxel.ai/release/cli/latest/\nsystem\n/moxel\n\n\n\n\nHere \nsystem\n is the operating system you are running with: \nosx\n, \nlinux\n or \nwindows\n.", 
            "title": "Cli"
        }, 
        {
            "location": "/install/cli/#installation-guide", 
            "text": "This guide explains how to install Moxel.", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/install/cli/#with-python-pip", 
            "text": "The easiest way to install Moxel regardless of the operating system is through  pip .   pip install moxel     This step will install two things:   Moxel CLI : a command-line tool to deploy machine learning models.  Moxel Python Client : this is a Python client to use Moxel models, through  import moxel .   After pip install, to test if the CLI is working, run  moxel login     If you are using a local computer, this step will open a web browser; if you currently ssh into a remote cluster,  moxel  will ask for username and passwords in headless mode.  You can also checkout the moxel version by   moxel version", 
            "title": "With Python PIP"
        }, 
        {
            "location": "/install/cli/#manually-install-cli", 
            "text": "Moxel CLI is released as standalone binaries. You can download them through  curl -o moxel http://beta.moxel.ai/release/cli/latest/ system /moxel  Here  system  is the operating system you are running with:  osx ,  linux  or  windows .", 
            "title": "Manually Install CLI"
        }, 
        {
            "location": "/tutorials/question-answering/", 
            "text": "Question-Answering\n\n\nIn this tutorial, you'll learn how to deploy a question-answering model trained on Stanford QA Dataset (SQuAD). Here's a sneak peak of the model demo:\n\n\n\n\nWhat is SQuAD?\n\n\nStanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.\nExplore SQuAD and model predictions\n\n\nCheckout the GitHub repo at \n\n\n\n\nhttps://rajpurkar.github.io/SQuAD-explorer/\n\n\n\n\nStep 1: Write Prediction\n\n\nFor this example, we'll use the code in \ngithub.com/moxel/tf-bi-att-flow\n, which already has the model inference part written out. \n\n\nCreate a new script called \nserve.py\n, and import from existing modules.\n\n\nfrom squad.demo_prepro import prepro\nfrom basic.demo_cli import Demo\n\n\n\n\nTo serve the model with Moxel, all you need to write is an inference function. Let's call it \nget_answer\n: \n\n\ndemo = Demo()\n\ndef get_answer(paragraph, question):\n    pq_prepro = prepro(paragraph, question)\n    return {\n        'answer': demo.run(pq_prepro)\n    }\n\n\n\n\nStep 2: Write the YAML config.\n\n\nCreate a new file called \nmoxel.yml\n. This Moxel config describes basic metadata about the model. \n\n\nname: question-answering\ntag: latest\nimage: py3-tf\nresources:\n  memory: 1Gi\n  cpu: \n1\n\ninput_space:\n  paragraph: str\n  question: str\noutput_space:\n  answer: str\nmain:\n  type: python\n  entrypoint: serve.py::get_answer\n\n\n\n\nIt provides a few specs about the model:\n\n\n\n\nname\n: the repo name under \n/\n.\n\n\ntag\n: versioning of the model.\n\n\nimage\n: the docker image to serve the model in.\n\n\nresources\n: basic resource requirements for the model. If not specified, it will request 1 CPU and 1 GB of memory.\n\n\ninput_space\n: the variables and their types for model input.\n\n\noutput_space\n: the variables and their types for model output.\n\n\nmain\n: the entrypoint to the model. For python deployment, specify the name of a function.\n\n\n\n\nStep 3 (Optional): Test it locally.\n\n\nBefore actually deploying the model, it's helpful to test and debug locally. Moxel provides \n\n\nmoxel serve -f moxel.yml\n\n\n\n\nThis will wrap the python function in a flask server, and start listening at port \n5900\n.\n\n\nTo use the model, create a test script called \ntest_server.py\n:\n\n\nimport moxel\n\nmodel = moxel.Model('\nusername\n/question-answering:latest', where='localhost')\n\noutput = model.predict(\n    paragraph='Daniel is 10 years old.', \n    question: 'How old is Daniel?'\n)\n\nprint(output['answer'])\n\n\n\n\nStep 4: Deploy!\n\n\nTo deploy the model, it's simple. Just run\n\n\nmoxel push -f moxel.yml\n\n\n\n\nYou'll see logs streamed live from the model. Shortly, the model will be available under \nbeta.moxel.ai/\\\nusername>/question-answer\n. \n\n\nOnce you go to the model, spend time to make the page look nice, by adding model description, readme, labels, ... Share it on Facebook / Twitter, and have your friends try it out!", 
            "title": "Question answering"
        }, 
        {
            "location": "/tutorials/question-answering/#question-answering", 
            "text": "In this tutorial, you'll learn how to deploy a question-answering model trained on Stanford QA Dataset (SQuAD). Here's a sneak peak of the model demo:", 
            "title": "Question-Answering"
        }, 
        {
            "location": "/tutorials/question-answering/#what-is-squad", 
            "text": "Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.\nExplore SQuAD and model predictions  Checkout the GitHub repo at    https://rajpurkar.github.io/SQuAD-explorer/", 
            "title": "What is SQuAD?"
        }, 
        {
            "location": "/tutorials/question-answering/#step-1-write-prediction", 
            "text": "For this example, we'll use the code in  github.com/moxel/tf-bi-att-flow , which already has the model inference part written out.   Create a new script called  serve.py , and import from existing modules.  from squad.demo_prepro import prepro\nfrom basic.demo_cli import Demo  To serve the model with Moxel, all you need to write is an inference function. Let's call it  get_answer :   demo = Demo()\n\ndef get_answer(paragraph, question):\n    pq_prepro = prepro(paragraph, question)\n    return {\n        'answer': demo.run(pq_prepro)\n    }", 
            "title": "Step 1: Write Prediction"
        }, 
        {
            "location": "/tutorials/question-answering/#step-2-write-the-yaml-config", 
            "text": "Create a new file called  moxel.yml . This Moxel config describes basic metadata about the model.   name: question-answering\ntag: latest\nimage: py3-tf\nresources:\n  memory: 1Gi\n  cpu:  1 \ninput_space:\n  paragraph: str\n  question: str\noutput_space:\n  answer: str\nmain:\n  type: python\n  entrypoint: serve.py::get_answer  It provides a few specs about the model:   name : the repo name under  / .  tag : versioning of the model.  image : the docker image to serve the model in.  resources : basic resource requirements for the model. If not specified, it will request 1 CPU and 1 GB of memory.  input_space : the variables and their types for model input.  output_space : the variables and their types for model output.  main : the entrypoint to the model. For python deployment, specify the name of a function.", 
            "title": "Step 2: Write the YAML config."
        }, 
        {
            "location": "/tutorials/question-answering/#step-3-optional-test-it-locally", 
            "text": "Before actually deploying the model, it's helpful to test and debug locally. Moxel provides   moxel serve -f moxel.yml  This will wrap the python function in a flask server, and start listening at port  5900 .  To use the model, create a test script called  test_server.py :  import moxel\n\nmodel = moxel.Model(' username /question-answering:latest', where='localhost')\n\noutput = model.predict(\n    paragraph='Daniel is 10 years old.', \n    question: 'How old is Daniel?'\n)\n\nprint(output['answer'])", 
            "title": "Step 3 (Optional): Test it locally."
        }, 
        {
            "location": "/tutorials/question-answering/#step-4-deploy", 
            "text": "To deploy the model, it's simple. Just run  moxel push -f moxel.yml  You'll see logs streamed live from the model. Shortly, the model will be available under  beta.moxel.ai/\\ username>/question-answer .   Once you go to the model, spend time to make the page look nice, by adding model description, readme, labels, ... Share it on Facebook / Twitter, and have your friends try it out!", 
            "title": "Step 4: Deploy!"
        }, 
        {
            "location": "/tutorials/mnist/", 
            "text": "MNIST with TensorFlow\n\n\nThis tutorial shows how to wrap a basic MNIST model in Tensorflow and upload it to Moxel.\n\n\nLive model: \nhttp://beta.moxel.ai/models/strin/mnist/latest\n\n\nSource code: \nhttps://github.com/moxel/tf-mnist\n.\n\n\nWrap Model in \nserve.py\n\n\nMoxel will upload your git repository to cloud, together with large model files such as weights. You simply need to write a \nserve.py\n file, which may import your existing modules.\n\n\nThe \nserve.py\n file first initializes global resources, such as the MNIST predictor, and some global constants.\n\n\nimport os\nimport moxel.space\n\nfrom MNISTTester import MNISTTester\n\nmnist = MNISTTester(\n            model_path='mnist/data/',\n            data_path='/models/mnist-cnn')\n\n\n\n\nEvery time someone calls your model, Moxel would handle the request with your \npredict\n function. In the MNIST case, the predict function is,\n\n\ndef predict(img):\n    img_bw = img.to_PIL().convert('L')\n    img_bw = moxel.space.Image.from_PIL(img_bw)\n    out = mnist.predict(img_bw.to_stream())\n    return {'out': out}\n\n\n\n\nThe function takes an input \nimg\n and produces one output \nout\n. Note that all \npredict\n functions must have a dict as output, with variable names being the keys.\n\n\nWrite Model Spec File \nmoxel.yml\n\n\nNow, write a model spec file that tells Moxel how to serve the model. Start a \nmoxel.yml\n:\n\n\nname: mnist\ntag: latest\nimage: py2-tf\nassets:\n- mnist/data\n- models\nresources:\n  memory: 512Mi\n  cpu: \n1\n\ninput_space:\n  img: image\noutput_space:\n  out: int\nmain:\n  type: python\n  entrypoint: serve.py::predict\n\n\n\n\nIn this example, we've specified the model should be run in \npy2-tf\n environment - Tensorflow 1.0 with Python 2. The input is \nimg\n of type \nimage\n, and the output is \nout\n of type int.\n\n\nThe \nmain\n tells Moxel how to load the model. The entrypoint \nserve.py::predict\n says we should process request with the \npredict\n function in \nserve.py\n.\n\n\nTest Model Locally\n\n\nMoxel allows you to test your model locally. This is as simple as \n\n\nmoxel serve -f moxel.yml\n\n\n\n\nThis will start a HTTP server locally, listening at \nlocalhost:5900\n.\n\n\nTo use the API locally, try \ntest.py\n\n\nimport moxel\n\nmodel = moxel.Model('strin/mnist:latest', where='localhost')\nimage = moxel.space.Image.from_file('imgs/digit-2-rgb.png')\ndigit = model.predict(img=image)\nprint 'digit=', digit\n\n\n\n\nRun \ntest.py\n and see if the output is \n2\n.\n\n\nPush Model to Moxel\n\n\nJust run \n\n\nmoxel push\n\n\n\n\nAfter a few seconds, your model would be live at Moxel, and you can play around the demo.", 
            "title": "Mnist"
        }, 
        {
            "location": "/tutorials/mnist/#mnist-with-tensorflow", 
            "text": "This tutorial shows how to wrap a basic MNIST model in Tensorflow and upload it to Moxel.  Live model:  http://beta.moxel.ai/models/strin/mnist/latest  Source code:  https://github.com/moxel/tf-mnist .", 
            "title": "MNIST with TensorFlow"
        }, 
        {
            "location": "/tutorials/mnist/#wrap-model-in-servepy", 
            "text": "Moxel will upload your git repository to cloud, together with large model files such as weights. You simply need to write a  serve.py  file, which may import your existing modules.  The  serve.py  file first initializes global resources, such as the MNIST predictor, and some global constants.  import os\nimport moxel.space\n\nfrom MNISTTester import MNISTTester\n\nmnist = MNISTTester(\n            model_path='mnist/data/',\n            data_path='/models/mnist-cnn')  Every time someone calls your model, Moxel would handle the request with your  predict  function. In the MNIST case, the predict function is,  def predict(img):\n    img_bw = img.to_PIL().convert('L')\n    img_bw = moxel.space.Image.from_PIL(img_bw)\n    out = mnist.predict(img_bw.to_stream())\n    return {'out': out}  The function takes an input  img  and produces one output  out . Note that all  predict  functions must have a dict as output, with variable names being the keys.", 
            "title": "Wrap Model in serve.py"
        }, 
        {
            "location": "/tutorials/mnist/#write-model-spec-file-moxelyml", 
            "text": "Now, write a model spec file that tells Moxel how to serve the model. Start a  moxel.yml :  name: mnist\ntag: latest\nimage: py2-tf\nassets:\n- mnist/data\n- models\nresources:\n  memory: 512Mi\n  cpu:  1 \ninput_space:\n  img: image\noutput_space:\n  out: int\nmain:\n  type: python\n  entrypoint: serve.py::predict  In this example, we've specified the model should be run in  py2-tf  environment - Tensorflow 1.0 with Python 2. The input is  img  of type  image , and the output is  out  of type int.  The  main  tells Moxel how to load the model. The entrypoint  serve.py::predict  says we should process request with the  predict  function in  serve.py .", 
            "title": "Write Model Spec File moxel.yml"
        }, 
        {
            "location": "/tutorials/mnist/#test-model-locally", 
            "text": "Moxel allows you to test your model locally. This is as simple as   moxel serve -f moxel.yml  This will start a HTTP server locally, listening at  localhost:5900 .  To use the API locally, try  test.py  import moxel\n\nmodel = moxel.Model('strin/mnist:latest', where='localhost')\nimage = moxel.space.Image.from_file('imgs/digit-2-rgb.png')\ndigit = model.predict(img=image)\nprint 'digit=', digit  Run  test.py  and see if the output is  2 .", 
            "title": "Test Model Locally"
        }, 
        {
            "location": "/tutorials/mnist/#push-model-to-moxel", 
            "text": "Just run   moxel push  After a few seconds, your model would be live at Moxel, and you can play around the demo.", 
            "title": "Push Model to Moxel"
        }, 
        {
            "location": "/moxel/moxel_yml_schema/", 
            "text": "moxel.yml schema:\n\n\nA Moxel YAML file looks like this: \n\n\nname: image_colorization\ntag: 0.0.1 \nimage: py3-tf\nassets: \n- ./models/colorization_deploy_v2.prototxt\n- ./models/colorization_release_v2.caffemodel\nresources:\n memory: 512Mi\n cpu: \n1\n\ninput_space:\n  img_in: image\noutput_space: \n  img_out: image\nmain:\n  type: python\n    entrypoint: serve.py::get_answer\n\n\n\n\nIt provides some specs about the model:\n\n\n\n\nname\n: the repo name for the model.\n\n\ntag\n: version of the model.\n\n\nimage\n: the docker image to serve the model in. Must be one of the \nsupported images\n.\n\n\nassets\n: paths to large assets that will be linked in the Docker container. These will be uploaded by the Moxel CLI even if they are not tracked in Git.\n\n\nresources\n: basic computing resource requirements for the model. If not specified, it will request 1 CPU and 1 GB of memory.\n\n\ninput_space\n: the variable names and their types for model input. Must be one of the \nsupported types\n.\n\n\noutput_space\n: the variable names and their types for model output. Must be one of the \nsupported types\n\n\nmain\n: the entrypoint to the model. For python deployment, specify the name of a function. At the moment only Python deployment is supported.", 
            "title": "Moxel yml schema"
        }, 
        {
            "location": "/moxel/moxel_yml_schema/#moxelyml-schema", 
            "text": "A Moxel YAML file looks like this:   name: image_colorization\ntag: 0.0.1 \nimage: py3-tf\nassets: \n- ./models/colorization_deploy_v2.prototxt\n- ./models/colorization_release_v2.caffemodel\nresources:\n memory: 512Mi\n cpu:  1 \ninput_space:\n  img_in: image\noutput_space: \n  img_out: image\nmain:\n  type: python\n    entrypoint: serve.py::get_answer  It provides some specs about the model:   name : the repo name for the model.  tag : version of the model.  image : the docker image to serve the model in. Must be one of the  supported images .  assets : paths to large assets that will be linked in the Docker container. These will be uploaded by the Moxel CLI even if they are not tracked in Git.  resources : basic computing resource requirements for the model. If not specified, it will request 1 CPU and 1 GB of memory.  input_space : the variable names and their types for model input. Must be one of the  supported types .  output_space : the variable names and their types for model output. Must be one of the  supported types  main : the entrypoint to the model. For python deployment, specify the name of a function. At the moment only Python deployment is supported.", 
            "title": "moxel.yml schema:"
        }, 
        {
            "location": "/moxel/supported_images/", 
            "text": "conda2\nconda3\npy2-caffe \npy2-pytorch \npy2-tf  \npy2-theano  \npy3-pytorch \npy3-tf-0.11\npy3-tf\npython2\npython3\n\n\n\n\nDon't see your framework here? Let us know on Intercom!", 
            "title": "Supported images"
        }, 
        {
            "location": "/moxel/io_types/", 
            "text": "IO Types\n\n\nMoxel standardizes the input and output spaces of models. It provides a set of basic types, such as images and text. \n\n\nTo use any of the Moxel Python classes in code, make sure you've installed Moxel and import from the moxel.space module:\n\n\nfrom moxel.space.image import Image\nfrom moxel.space.json import JSON\n\n\n\n\nDetailed specs are below:\n\n\nString\n\n\n\n\nYAML schema name\n: str \n\n\nPython type\n: str\n\n\n\n\nImage\n\n\n\n\n\n\nYAML schema name\n: image\n\n\n\n\n\n\nPython type\n:\n\n\n\n\n\n\nfrom .core import Space\nimport numpy as np\nimport base64\nfrom six import BytesIO\n\n\nclass Image(Space):\n    \n Logical semantic space for images.\n    \n\n    NAME='Image'\n\n    def __init__(self, im):\n        self.im = im\n\n    def resize(self, size, interp='bilinear', mode=None):\n        import scipy.misc\n        self.im = scipy.misc.imresize(self.im, size, interp, mode)\n\n    @property\n    def shape(self):\n        return self.im.shape\n\n    def rgb(self, i, j):\n        return self.im[i, j, :] * 255\n\n    @staticmethod\n    def from_stream(f):\n        # import only if the transformer is used.\n        from PIL import Image as PILImage\n\n        image = PILImage.open(f)\n        return Image.from_PIL(image)\n\n    @staticmethod\n    def from_base64(data):\n        import base64\n        from six import BytesIO\n\n        image_binary = base64.b64decode(data)\n\n        image_f = BytesIO()\n        image_f.write(image_binary)\n        image_f.seek(0)\n\n        return Image.from_stream(image_f)\n\n    @staticmethod\n    def from_file(filename):\n        with open(filename, 'rb') as f:\n            return Image.from_stream(f)\n\n    @staticmethod\n    def from_PIL(image):\n        image_data = np.array(image.getdata())\n\n        (im_width, im_height) = image.size\n        im_channel = len(image.getbands())\n\n        if im_channel == 1:\n            im = image_data.reshape((im_height, im_width)) / 255.\n        else:\n            im = image_data.reshape((im_height, im_width, im_channel)) / 255.\n\n        return Image(im)\n\n\n\n    def to_numpy(self):\n        return self.im\n\n    def to_numpy_rgb(self):\n        return self.im * 255\n\n    def to_PIL(self):\n        from PIL import Image as PILImage\n        return PILImage.fromarray(np.array(self.im * 255, dtype='uint8'))\n\n    def to_stream(self):\n        \n Return a stream of the image\n        \n\n        image_pil = self.to_PIL()\n        buf = BytesIO()\n        image_pil.save(buf, 'png')\n        buf.seek(0)\n        return buf\n\n    def to_base64(self):\n        \n Return png of the image in base64 encoding.\n        \n\n        buf = self.to_stream()\n        return base64.b64encode(buf.read()).decode('utf_8')\n\n    def __repr__(self):\n        return '\nMoxel Image {}\n'.format(self.shape)\n\n\n\n\nJSON\n\n\n\n\n\n\nYAML schema name\n: json\n\n\n\n\n\n\nPython type:\n\n\n\n\n\n\nfrom .core import Space\n\nimport json\n\n\nclass JSON(Space):\n    NAME = 'JSON'\n\n    def __init__(self, data):\n        self.data = data\n\n    @staticmethod\n    def from_object(json):\n        assert isinstance(json, dict) or isinstance(json, list)\n        return JSON(json)\n\n    def to_str(self):\n        return json.dumps(self.data)\n\n    def to_bytes(self, encoding='utf_8'):\n        return self.to_str().encode(encoding)\n\n    def to_object(self):\n        return self.data", 
            "title": "Io types"
        }, 
        {
            "location": "/moxel/io_types/#io-types", 
            "text": "Moxel standardizes the input and output spaces of models. It provides a set of basic types, such as images and text.   To use any of the Moxel Python classes in code, make sure you've installed Moxel and import from the moxel.space module:  from moxel.space.image import Image\nfrom moxel.space.json import JSON  Detailed specs are below:", 
            "title": "IO Types"
        }, 
        {
            "location": "/moxel/io_types/#string", 
            "text": "YAML schema name : str   Python type : str", 
            "title": "String"
        }, 
        {
            "location": "/moxel/io_types/#image", 
            "text": "YAML schema name : image    Python type :    from .core import Space\nimport numpy as np\nimport base64\nfrom six import BytesIO\n\n\nclass Image(Space):\n      Logical semantic space for images.\n     \n    NAME='Image'\n\n    def __init__(self, im):\n        self.im = im\n\n    def resize(self, size, interp='bilinear', mode=None):\n        import scipy.misc\n        self.im = scipy.misc.imresize(self.im, size, interp, mode)\n\n    @property\n    def shape(self):\n        return self.im.shape\n\n    def rgb(self, i, j):\n        return self.im[i, j, :] * 255\n\n    @staticmethod\n    def from_stream(f):\n        # import only if the transformer is used.\n        from PIL import Image as PILImage\n\n        image = PILImage.open(f)\n        return Image.from_PIL(image)\n\n    @staticmethod\n    def from_base64(data):\n        import base64\n        from six import BytesIO\n\n        image_binary = base64.b64decode(data)\n\n        image_f = BytesIO()\n        image_f.write(image_binary)\n        image_f.seek(0)\n\n        return Image.from_stream(image_f)\n\n    @staticmethod\n    def from_file(filename):\n        with open(filename, 'rb') as f:\n            return Image.from_stream(f)\n\n    @staticmethod\n    def from_PIL(image):\n        image_data = np.array(image.getdata())\n\n        (im_width, im_height) = image.size\n        im_channel = len(image.getbands())\n\n        if im_channel == 1:\n            im = image_data.reshape((im_height, im_width)) / 255.\n        else:\n            im = image_data.reshape((im_height, im_width, im_channel)) / 255.\n\n        return Image(im)\n\n\n\n    def to_numpy(self):\n        return self.im\n\n    def to_numpy_rgb(self):\n        return self.im * 255\n\n    def to_PIL(self):\n        from PIL import Image as PILImage\n        return PILImage.fromarray(np.array(self.im * 255, dtype='uint8'))\n\n    def to_stream(self):\n          Return a stream of the image\n         \n        image_pil = self.to_PIL()\n        buf = BytesIO()\n        image_pil.save(buf, 'png')\n        buf.seek(0)\n        return buf\n\n    def to_base64(self):\n          Return png of the image in base64 encoding.\n         \n        buf = self.to_stream()\n        return base64.b64encode(buf.read()).decode('utf_8')\n\n    def __repr__(self):\n        return ' Moxel Image {} '.format(self.shape)", 
            "title": "Image"
        }, 
        {
            "location": "/moxel/io_types/#json", 
            "text": "YAML schema name : json    Python type:    from .core import Space\n\nimport json\n\n\nclass JSON(Space):\n    NAME = 'JSON'\n\n    def __init__(self, data):\n        self.data = data\n\n    @staticmethod\n    def from_object(json):\n        assert isinstance(json, dict) or isinstance(json, list)\n        return JSON(json)\n\n    def to_str(self):\n        return json.dumps(self.data)\n\n    def to_bytes(self, encoding='utf_8'):\n        return self.to_str().encode(encoding)\n\n    def to_object(self):\n        return self.data", 
            "title": "JSON"
        }, 
        {
            "location": "/moxel/serve/", 
            "text": "Serve Models Locally\n\n\nYou can serve models locally before uploading them to Moxel.\n\n\nServe Models without Docker\n\n\nTo serve this model locally, simply run \n\n\nmoxel serve -f moxel.yml\n\n\n\n\nYou will see Moxel starts \nmoxel-python-server\n and loads \npredict\n function:\n\n\nUsing default resource setting map[cpu:1 memory:1Gi]\nLocally serving model moxel/hello:latest.\nCommand: [moxel-python-driver --json {\nasset_root\n:\n/Users/tims/moxel/moxel/examples/perceptron\n,\nassets\n:[],\ncode_root\n:\n/Users/tims/moxel/moxel/examples/perceptron\n,\nentrypoint\n:\nserve.py::predict\n,\ninput_space\n:{\nsentence\n:\nstr\n},\noutput_space\n:{\nsentiment\n:\nstr\n},\nsetup\n:[],\nwork_path\n:\n.\n}]\nPython driver version 0.0.2\nLoaded prediction function \nfunction predict at 0x1073f8c80\n\n * Running on http://0.0.0.0:5900/ (Press CTRL+C to quit)\n\n\n\n\nServe Models with Docker\n\n\nThis is as simple as \n\n\nmoxel serve -f moxel.yml --docker\n\n\n\n\nThe docker will use \nimage\n in the model yaml spec as Docker image .\n\n\nTesting locally\n\n\nTo test one of your local endpoints created above, create a test script:\n\n\nimport moxel\n\nmodel = moxel.Model('\nusername\n/question-answering:latest', where='localhost')\n\noutput = model.predict(\n    paragraph='Daniel is 10 years old.', \n        question: 'How old is Daniel?'\n        )\n\n        print(output['answer'])\n\n\n\n\nDebug Moxel Drivers\n\n\nTo debug moxel drivers, set \nDRIVER_DEV=1\n environment variable when serving models locally.\n\n\nDRIVER_DEV=1 moxel serve -f moxel.yml --docker", 
            "title": "Serve"
        }, 
        {
            "location": "/moxel/serve/#serve-models-locally", 
            "text": "You can serve models locally before uploading them to Moxel.", 
            "title": "Serve Models Locally"
        }, 
        {
            "location": "/moxel/serve/#serve-models-without-docker", 
            "text": "To serve this model locally, simply run   moxel serve -f moxel.yml  You will see Moxel starts  moxel-python-server  and loads  predict  function:  Using default resource setting map[cpu:1 memory:1Gi]\nLocally serving model moxel/hello:latest.\nCommand: [moxel-python-driver --json { asset_root : /Users/tims/moxel/moxel/examples/perceptron , assets :[], code_root : /Users/tims/moxel/moxel/examples/perceptron , entrypoint : serve.py::predict , input_space :{ sentence : str }, output_space :{ sentiment : str }, setup :[], work_path : . }]\nPython driver version 0.0.2\nLoaded prediction function  function predict at 0x1073f8c80 \n * Running on http://0.0.0.0:5900/ (Press CTRL+C to quit)", 
            "title": "Serve Models without Docker"
        }, 
        {
            "location": "/moxel/serve/#serve-models-with-docker", 
            "text": "This is as simple as   moxel serve -f moxel.yml --docker  The docker will use  image  in the model yaml spec as Docker image .", 
            "title": "Serve Models with Docker"
        }, 
        {
            "location": "/moxel/serve/#testing-locally", 
            "text": "To test one of your local endpoints created above, create a test script:  import moxel\n\nmodel = moxel.Model(' username /question-answering:latest', where='localhost')\n\noutput = model.predict(\n    paragraph='Daniel is 10 years old.', \n        question: 'How old is Daniel?'\n        )\n\n        print(output['answer'])", 
            "title": "Testing locally"
        }, 
        {
            "location": "/moxel/serve/#debug-moxel-drivers", 
            "text": "To debug moxel drivers, set  DRIVER_DEV=1  environment variable when serving models locally.  DRIVER_DEV=1 moxel serve -f moxel.yml --docker", 
            "title": "Debug Moxel Drivers"
        }
    ]
}