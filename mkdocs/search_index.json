{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to Moxel Documentation.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-moxel-documentation",
            "text": "",
            "title": "Welcome to Moxel Documentation."
        },
        {
            "location": "/install/cli/",
            "text": "Installation Guide\n\n\nThis guide explains how to install Moxel.\n\n\nWith Python PIP\n\n\nThe easiest way to install Moxel regardless of the operating system is through \npip\n. \n\n\npip install moxel\n\n\n\n\n\n\n\n\n\n\nThis step will install two things:\n\n\n\n\nMoxel CLI\n: a command-line tool to deploy machine learning models.\n\n\nMoxel Python Client\n: this is a Python client to use Moxel models, through \nimport moxel\n.\n\n\n\n\nAfter pip install, to test if the CLI is working, run\n\n\nmoxel login\n\n\n\n\n\n\n\n\n\n\nIf you are using a local computer, this step will open a web browser; if you currently ssh into a remote cluster, \nmoxel\n will ask for username and passwords in headless mode.\n\n\nYou can also checkout the moxel version by \n\n\nmoxel version\n\n\n\n\nManually Install CLI\n\n\nMoxel CLI is released as standalone binaries. You can download them through\n\n\ncurl -o moxel http://beta.moxel.ai/release/cli/latest/<system>/moxel\n\n\n\n\nHere \n<system>\n is the operating system you are running with: \nosx\n, \nlinux\n or \nwindows\n.",
            "title": "Cli"
        },
        {
            "location": "/install/cli/#installation-guide",
            "text": "This guide explains how to install Moxel.",
            "title": "Installation Guide"
        },
        {
            "location": "/install/cli/#with-python-pip",
            "text": "The easiest way to install Moxel regardless of the operating system is through  pip .   pip install moxel     This step will install two things:   Moxel CLI : a command-line tool to deploy machine learning models.  Moxel Python Client : this is a Python client to use Moxel models, through  import moxel .   After pip install, to test if the CLI is working, run  moxel login     If you are using a local computer, this step will open a web browser; if you currently ssh into a remote cluster,  moxel  will ask for username and passwords in headless mode.  You can also checkout the moxel version by   moxel version",
            "title": "With Python PIP"
        },
        {
            "location": "/install/cli/#manually-install-cli",
            "text": "Moxel CLI is released as standalone binaries. You can download them through  curl -o moxel http://beta.moxel.ai/release/cli/latest/<system>/moxel  Here  <system>  is the operating system you are running with:  osx ,  linux  or  windows .",
            "title": "Manually Install CLI"
        },
        {
            "location": "/tutorials/question-answering/",
            "text": "Question-Answering\n\n\nIn this tutorial, you'll learn how to deploy a question-answering model trained on Stanford QA Dataset (SQuAD). Here's a sneak peak of the model demo:\n\n\n\n\nWhat is SQuAD?\n\n\nStanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.\nExplore SQuAD and model predictions\n\n\nCheckout the GitHub repo at \n\n\n\n\nhttps://rajpurkar.github.io/SQuAD-explorer/\n\n\n\n\nStep 1: Write Prediction\n\n\nFor this example, we'll use the code in \ngithub.com/moxel/tf-bi-att-flow\n, which already has the model inference part written out. \n\n\nCreate a new script called \nserve.py\n, and import from existing modules.\n\n\nfrom squad.demo_prepro import prepro\nfrom basic.demo_cli import Demo\n\n\n\n\nTo serve the model with Moxel, all you need to write is an inference function. Let's call it \nget_answer\n: \n\n\ndemo = Demo()\n\ndef get_answer(paragraph, question):\n    pq_prepro = prepro(paragraph, question)\n    return {\n        'answer': demo.run(pq_prepro)\n    }\n\n\n\n\nStep 2: Write the YAML config.\n\n\nCreate a new file called \nmoxel.yml\n. This Moxel config describes basic metadata about the model. \n\n\nname: question-answering\ntag: latest\nimage: dummyai/py3-tf-cpu\nresources:\n  memory: 1Gi\n  cpu: \"1\"\ninput_space:\n  paragraph: String\n  question: String\noutput_space:\n  answer: String\nmain:\n  type: python\n  entrypoint: serve.py::get_answer\n\n\n\n\nIt provides a few specs about the model:\n\n\n\n\nname\n: the repo name under \n/\n.\n\n\ntag\n: versioning of the model.\n\n\nimage\n: the docker image to serve the model in.\n\n\nresources\n: basic resource requirements for the model. If not specified, it will request 1 CPU and 1 GB of memory.\n\n\ninput_space\n: the variables and their types for model input.\n\n\noutput_space\n: the variables and their types for model output.\n\n\nmain\n: the entrypoint to the model. For python deployment, specify the name of a function.\n\n\n\n\nStep 3 (Optional): Test it locally.\n\n\nBefore actually deploying the model, it's helpful to test and debug locally. Moxel provides \n\n\nmoxel serve -f moxel.yml\n\n\n\n\nThis will wrap the python function in a flask server, and start listening at port \n5900\n.\n\n\nTo use the model, create a test script called \ntest_server.py\n:\n\n\nimport moxel\n\nmodel = moxel.Model('<username>/question-answering:latest', where='localhost')\n\noutput = model.predict({\n    'paragraph': 'Daniel is 10 years old.',\n    'question': 'How old is Daniel?'\n})\n\nprint(output['answer'])\n\n\n\n\nStep 4: Deploy!\n\n\nTo deploy the model, it's simple. Just run\n\n\nmoxel push -f moxel.yml\n\n\n\n\nYou'll see logs streamed live from the model. Shortly, the model will be available under \nbeta.moxel.ai/\n/question-answer\n. \n\n\nOnce you go to the model, spend time to make the page look nice, by adding model description, readme, labels, ... Share it on Facebook / Twitter, and have your friends try it out!",
            "title": "Question answering"
        },
        {
            "location": "/tutorials/question-answering/#question-answering",
            "text": "In this tutorial, you'll learn how to deploy a question-answering model trained on Stanford QA Dataset (SQuAD). Here's a sneak peak of the model demo:",
            "title": "Question-Answering"
        },
        {
            "location": "/tutorials/question-answering/#what-is-squad",
            "text": "Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.\nExplore SQuAD and model predictions  Checkout the GitHub repo at    https://rajpurkar.github.io/SQuAD-explorer/",
            "title": "What is SQuAD?"
        },
        {
            "location": "/tutorials/question-answering/#step-1-write-prediction",
            "text": "For this example, we'll use the code in  github.com/moxel/tf-bi-att-flow , which already has the model inference part written out.   Create a new script called  serve.py , and import from existing modules.  from squad.demo_prepro import prepro\nfrom basic.demo_cli import Demo  To serve the model with Moxel, all you need to write is an inference function. Let's call it  get_answer :   demo = Demo()\n\ndef get_answer(paragraph, question):\n    pq_prepro = prepro(paragraph, question)\n    return {\n        'answer': demo.run(pq_prepro)\n    }",
            "title": "Step 1: Write Prediction"
        },
        {
            "location": "/tutorials/question-answering/#step-2-write-the-yaml-config",
            "text": "Create a new file called  moxel.yml . This Moxel config describes basic metadata about the model.   name: question-answering\ntag: latest\nimage: dummyai/py3-tf-cpu\nresources:\n  memory: 1Gi\n  cpu: \"1\"\ninput_space:\n  paragraph: String\n  question: String\noutput_space:\n  answer: String\nmain:\n  type: python\n  entrypoint: serve.py::get_answer  It provides a few specs about the model:   name : the repo name under  / .  tag : versioning of the model.  image : the docker image to serve the model in.  resources : basic resource requirements for the model. If not specified, it will request 1 CPU and 1 GB of memory.  input_space : the variables and their types for model input.  output_space : the variables and their types for model output.  main : the entrypoint to the model. For python deployment, specify the name of a function.",
            "title": "Step 2: Write the YAML config."
        },
        {
            "location": "/tutorials/question-answering/#step-3-optional-test-it-locally",
            "text": "Before actually deploying the model, it's helpful to test and debug locally. Moxel provides   moxel serve -f moxel.yml  This will wrap the python function in a flask server, and start listening at port  5900 .  To use the model, create a test script called  test_server.py :  import moxel\n\nmodel = moxel.Model('<username>/question-answering:latest', where='localhost')\n\noutput = model.predict({\n    'paragraph': 'Daniel is 10 years old.',\n    'question': 'How old is Daniel?'\n})\n\nprint(output['answer'])",
            "title": "Step 3 (Optional): Test it locally."
        },
        {
            "location": "/tutorials/question-answering/#step-4-deploy",
            "text": "To deploy the model, it's simple. Just run  moxel push -f moxel.yml  You'll see logs streamed live from the model. Shortly, the model will be available under  beta.moxel.ai/ /question-answer .   Once you go to the model, spend time to make the page look nice, by adding model description, readme, labels, ... Share it on Facebook / Twitter, and have your friends try it out!",
            "title": "Step 4: Deploy!"
        },
        {
            "location": "/tutorials/caffe-colorization/",
            "text": "Image Colorization in Caffe\n\n\n\n\nAutomatic colorization using deep neural networks. \"Colorful Image Colorization.\" In ECCV, 2016. \nhttp://richzhang.github.io/colorization/\n.\n\n\nOverview\n\n\nThe first step is to make sure you've logged in. Try listing your models:\n\n\nmoxel list\n\n\n\n\nIf you haven't logged in yet, run the following command and you would be redirected to the login portal in browser.\n\n\nmoxel login\n\n\n\n\nSimilar to Github, Moxel hosts model in \nrepos\n. A model repo is uniquely identified as \n<userName>/<modelName>\n. A model can have multiple versions, just like git version controls code. We assign a tag to the model, and label it as \n\n\n<userName>/<modelName>:<tag>\n\n\n\n\nCreate a Model Repo\n\n\nJust like Github, you can create a model repo on Moxel website. Go to \nbeta.dummy.ai/new\n\n\n\n\nAfter filling out the model name and a one-line pitch, you will see the model page. It is easy to edit things like the model title, the description, README files, ... \n\n\n\n\nDeploy the Model\n\n\nStep 1\n. \n\n\nCreate a flask server that serves the following prediction procedure.\n\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef health_check():\n    return jsonify({\n        'status': 'OK'\n    })\n\n\n@app.route('/', methods=['POST'])\ndef detect():\n    data = request.json\n\n    image_binary = base64.b64decode(data['img_in'])\n\n    image_f = BytesIO()\n    image_f.write(image_binary)\n    image_f.seek(0)\n\n    image = Image.open(image_f)\n    image_np = load_image_into_numpy_array(image)\n\n    img_out = model.predict(image_np)['img_out']\n    vis_file = BytesIO()\n    scipy.misc.imsave(vis_file, img_out, format='png')\n    vis_file.seek(0)\n    vis_binary = vis_file.read()\n\n    return jsonify({\n        'img_out': base64.b64encode(vis_binary).decode('utf-8'),\n    })\n\nif __name__ == '__main__':\n    app.run(debug=False, port=5900, host='0.0.0.0')\n\n\n\n\nStep 2\n.\n\n\nWrite down \nmoxel.yml\n\n\nimage: dummyai/py3-caffe-cpu\nassets:\n- ./models/colorization_deploy_v2.prototxt\n- ./models/colorization_release_v2.caffemodel\n- ./resources/pts_in_hull.npy\nresources:\n  memory: 512Mi\n  cpu: \"1\"\ninput_space:\n  img_in: Image\noutput_space:\n  img_out: Image\ncmd:\n- pip install -r requirements.txt\n- python serve_model.py\n\n\n\n\nStep 3\n. \n\n\nPush the model to Moxel, and have it deployed.\n\n\nmoxel push -f moxel.yml colorization:latest\n\n\n\n\nBy default, the tag \nlatest\n is added when the model repo is created. You may also push to other versions, such as \n0.0.1\n. Moxel allows you to switch to different versions easily.",
            "title": "Caffe colorization"
        },
        {
            "location": "/tutorials/caffe-colorization/#image-colorization-in-caffe",
            "text": "Automatic colorization using deep neural networks. \"Colorful Image Colorization.\" In ECCV, 2016.  http://richzhang.github.io/colorization/ .",
            "title": "Image Colorization in Caffe"
        },
        {
            "location": "/tutorials/caffe-colorization/#overview",
            "text": "The first step is to make sure you've logged in. Try listing your models:  moxel list  If you haven't logged in yet, run the following command and you would be redirected to the login portal in browser.  moxel login  Similar to Github, Moxel hosts model in  repos . A model repo is uniquely identified as  <userName>/<modelName> . A model can have multiple versions, just like git version controls code. We assign a tag to the model, and label it as   <userName>/<modelName>:<tag>",
            "title": "Overview"
        },
        {
            "location": "/tutorials/caffe-colorization/#create-a-model-repo",
            "text": "Just like Github, you can create a model repo on Moxel website. Go to  beta.dummy.ai/new   After filling out the model name and a one-line pitch, you will see the model page. It is easy to edit things like the model title, the description, README files, ...",
            "title": "Create a Model Repo"
        },
        {
            "location": "/tutorials/caffe-colorization/#deploy-the-model",
            "text": "Step 1 .   Create a flask server that serves the following prediction procedure.  app = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef health_check():\n    return jsonify({\n        'status': 'OK'\n    })\n\n\n@app.route('/', methods=['POST'])\ndef detect():\n    data = request.json\n\n    image_binary = base64.b64decode(data['img_in'])\n\n    image_f = BytesIO()\n    image_f.write(image_binary)\n    image_f.seek(0)\n\n    image = Image.open(image_f)\n    image_np = load_image_into_numpy_array(image)\n\n    img_out = model.predict(image_np)['img_out']\n    vis_file = BytesIO()\n    scipy.misc.imsave(vis_file, img_out, format='png')\n    vis_file.seek(0)\n    vis_binary = vis_file.read()\n\n    return jsonify({\n        'img_out': base64.b64encode(vis_binary).decode('utf-8'),\n    })\n\nif __name__ == '__main__':\n    app.run(debug=False, port=5900, host='0.0.0.0')  Step 2 .  Write down  moxel.yml  image: dummyai/py3-caffe-cpu\nassets:\n- ./models/colorization_deploy_v2.prototxt\n- ./models/colorization_release_v2.caffemodel\n- ./resources/pts_in_hull.npy\nresources:\n  memory: 512Mi\n  cpu: \"1\"\ninput_space:\n  img_in: Image\noutput_space:\n  img_out: Image\ncmd:\n- pip install -r requirements.txt\n- python serve_model.py  Step 3 .   Push the model to Moxel, and have it deployed.  moxel push -f moxel.yml colorization:latest  By default, the tag  latest  is added when the model repo is created. You may also push to other versions, such as  0.0.1 . Moxel allows you to switch to different versions easily.",
            "title": "Deploy the Model"
        }
    ]
}